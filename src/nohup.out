2024-04-22,04:46:28 | INFO | Running in distributed mode with multiple processes. Device: cuda:0.Process (global: 0, local 0), total 4.
2024-04-22,04:46:28 | INFO | Loaded coca_ViT-B-32 model config.
2024-04-22,04:46:28 | INFO | Running in distributed mode with multiple processes. Device: cuda:1.Process (global: 1, local 1), total 4.
2024-04-22,04:46:28 | INFO | Loaded coca_ViT-B-32 model config.
2024-04-22,04:46:28 | INFO | Running in distributed mode with multiple processes. Device: cuda:3.Process (global: 3, local 3), total 4.
2024-04-22,04:46:28 | INFO | Loaded coca_ViT-B-32 model config.
2024-04-22,04:46:29 | INFO | Running in distributed mode with multiple processes. Device: cuda:2.Process (global: 2, local 2), total 4.
2024-04-22,04:46:29 | INFO | Loaded coca_ViT-B-32 model config.
2024-04-22,04:46:32 | INFO | Model:
2024-04-22,04:46:32 | INFO | CoCa(
  (text): TextTransformer(
    (token_embedding): Embedding(49408, 512)
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0-11): 12 x ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (visual): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0-11): 12 x ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (attn_pool): AttentionalPooler(
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (ln_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ln_k): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (ln_post): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (text_decoder): MultimodalTransformer(
    (resblocks): ModuleList(
      (0-11): 12 x ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
    )
    (cross_attn): ModuleList(
      (0-11): 12 x ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_1_kv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
    )
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
2024-04-22,04:46:32 | INFO | Params:
2024-04-22,04:46:32 | INFO |   accum_freq: 1
2024-04-22,04:46:32 | INFO |   aug_cfg: {}
2024-04-22,04:46:32 | INFO |   batch_size: 128
2024-04-22,04:46:32 | INFO |   beta1: 0.9
2024-04-22,04:46:32 | INFO |   beta2: 0.98
2024-04-22,04:46:32 | INFO |   checkpoint_path: ./logs/coca_coyo_elr_10/checkpoints
2024-04-22,04:46:32 | INFO |   coca_caption_loss_weight: 2.0
2024-04-22,04:46:32 | INFO |   coca_contrastive_loss_weight: 1.0
2024-04-22,04:46:32 | INFO |   copy_codebase: False
2024-04-22,04:46:32 | INFO |   csv_caption_key: title
2024-04-22,04:46:32 | INFO |   csv_img_key: filepath
2024-04-22,04:46:32 | INFO |   csv_separator: 	
2024-04-22,04:46:32 | INFO |   dataset_resampled: False
2024-04-22,04:46:32 | INFO |   dataset_type: webdataset
2024-04-22,04:46:32 | INFO |   ddp_static_graph: False
2024-04-22,04:46:32 | INFO |   debug: False
2024-04-22,04:46:32 | INFO |   delete_previous_checkpoint: False
2024-04-22,04:46:32 | INFO |   device: cuda:0
2024-04-22,04:46:32 | INFO |   dist_backend: nccl
2024-04-22,04:46:32 | INFO |   dist_url: env://
2024-04-22,04:46:32 | INFO |   distill: False
2024-04-22,04:46:32 | INFO |   distill_model: None
2024-04-22,04:46:32 | INFO |   distill_pretrained: None
2024-04-22,04:46:32 | INFO |   distributed: True
2024-04-22,04:46:32 | INFO |   elr_distill: True
2024-04-22,04:46:32 | INFO |   elr_ema_decay: 0.997
2024-04-22,04:46:32 | INFO |   elr_teacher_warmup: 3
2024-04-22,04:46:32 | INFO |   elr_weight: 3.0
2024-04-22,04:46:32 | INFO |   epochs: 100
2024-04-22,04:46:32 | INFO |   epochs_cooldown: None
2024-04-22,04:46:32 | INFO |   eps: 1e-06
2024-04-22,04:46:32 | INFO |   force_custom_text: False
2024-04-22,04:46:32 | INFO |   force_image_size: None
2024-04-22,04:46:32 | INFO |   force_patch_dropout: None
2024-04-22,04:46:32 | INFO |   force_quick_gelu: False
2024-04-22,04:46:32 | INFO |   gather_with_grad: True
2024-04-22,04:46:32 | INFO |   grad_checkpointing: False
2024-04-22,04:46:32 | INFO |   grad_clip_norm: None
2024-04-22,04:46:32 | INFO |   horovod: False
2024-04-22,04:46:32 | INFO |   image_interpolation: None
2024-04-22,04:46:32 | INFO |   image_mean: None
2024-04-22,04:46:32 | INFO |   image_resize_mode: None
2024-04-22,04:46:32 | INFO |   image_std: None
2024-04-22,04:46:32 | INFO |   imagenet_v2: None
2024-04-22,04:46:32 | INFO |   imagenet_val: /home/sandeepmukh/open_clip/imagenet/validation
2024-04-22,04:46:32 | INFO |   local_loss: True
2024-04-22,04:46:32 | INFO |   local_rank: 0
2024-04-22,04:46:32 | INFO |   lock_image: False
2024-04-22,04:46:32 | INFO |   lock_image_freeze_bn_stats: False
2024-04-22,04:46:32 | INFO |   lock_image_unlocked_groups: 0
2024-04-22,04:46:32 | INFO |   lock_text: False
2024-04-22,04:46:32 | INFO |   lock_text_freeze_layer_norm: False
2024-04-22,04:46:32 | INFO |   lock_text_unlocked_layers: 0
2024-04-22,04:46:32 | INFO |   log_every_n_steps: 100
2024-04-22,04:46:32 | INFO |   log_level: 20
2024-04-22,04:46:32 | INFO |   log_local: False
2024-04-22,04:46:32 | INFO |   log_path: ./logs/coca_coyo_elr_10/out.log
2024-04-22,04:46:32 | INFO |   logs: ./logs/
2024-04-22,04:46:32 | INFO |   lr: 0.0005
2024-04-22,04:46:32 | INFO |   lr_cooldown_end: 0.0
2024-04-22,04:46:32 | INFO |   lr_cooldown_power: 1.0
2024-04-22,04:46:32 | INFO |   lr_scheduler: cosine
2024-04-22,04:46:32 | INFO |   max_elr_bn_batches: 5
2024-04-22,04:46:32 | INFO |   model: coca_ViT-B-32
2024-04-22,04:46:32 | INFO |   name: coca_coyo_elr_10
2024-04-22,04:46:32 | INFO |   no_set_device_rank: False
2024-04-22,04:46:32 | INFO |   precision: amp
2024-04-22,04:46:32 | INFO |   pretrained: 
2024-04-22,04:46:32 | INFO |   pretrained_image: False
2024-04-22,04:46:32 | INFO |   rank: 0
2024-04-22,04:46:32 | INFO |   remote_sync: None
2024-04-22,04:46:32 | INFO |   remote_sync_frequency: 300
2024-04-22,04:46:32 | INFO |   remote_sync_protocol: s3
2024-04-22,04:46:32 | INFO |   report_to: wandb
2024-04-22,04:46:32 | INFO |   resume: None
2024-04-22,04:46:32 | INFO |   save_frequency: 5
2024-04-22,04:46:32 | INFO |   save_most_recent: False
2024-04-22,04:46:32 | INFO |   seed: 0
2024-04-22,04:46:32 | INFO |   siglip: False
2024-04-22,04:46:32 | INFO |   skip_scheduler: False
2024-04-22,04:46:32 | INFO |   tensorboard: False
2024-04-22,04:46:32 | INFO |   tensorboard_path: 
2024-04-22,04:46:32 | INFO |   torchcompile: True
2024-04-22,04:46:32 | INFO |   torchscript: False
2024-04-22,04:46:32 | INFO |   trace: False
2024-04-22,04:46:32 | INFO |   train_data: /home/sandeepmukh/imagen-pytorch/data/coyo-700m-webdataset/{00000..00340}.tar
2024-04-22,04:46:32 | INFO |   train_data_upsampling_factors: None
2024-04-22,04:46:32 | INFO |   train_num_samples: 1000000
2024-04-22,04:46:32 | INFO |   use_bn_sync: False
2024-04-22,04:46:32 | INFO |   use_bnb_linear: None
2024-04-22,04:46:32 | INFO |   val_data: /home/sandeepmukh/imagen-pytorch/data/coyo-700m-webdataset/{00340..00345}.tar
2024-04-22,04:46:32 | INFO |   val_frequency: 1
2024-04-22,04:46:32 | INFO |   val_num_samples: 10000
2024-04-22,04:46:32 | INFO |   wandb: True
2024-04-22,04:46:32 | INFO |   wandb_notes: 
2024-04-22,04:46:32 | INFO |   wandb_project_name: open-clip-elr
2024-04-22,04:46:32 | INFO |   warmup: 2000
2024-04-22,04:46:32 | INFO |   wd: 0.2
2024-04-22,04:46:32 | INFO |   workers: 4
2024-04-22,04:46:32 | INFO |   world_size: 4
2024-04-22,04:46:32 | INFO |   zeroshot_frequency: 2
libibverbs: Warning: couldn't load driver 'libmlx5-rdmav34.so': libmlx5-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libmlx5-rdmav34.so': libmlx5-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libmlx5-rdmav34.so': libmlx5-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libmlx5-rdmav34.so': libmlx5-rdmav34.so: cannot open shared object file: No such file or directory
2024-04-22,04:46:34 | INFO | Compiling model...
2024-04-22,04:46:34 | INFO | Compiling model...
2024-04-22,04:46:34 | INFO | Compiling model...
wandb: Currently logged in as: sandeep-m (space-narwhals). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home/sandeepmukh/open_clip/src/wandb/run-20240422_044635-coca_coyo_elr_10
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run coca_coyo_elr_10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/space-narwhals/open-clip-elr
wandb: üöÄ View run at https://wandb.ai/space-narwhals/open-clip-elr/runs/coca_coyo_elr_10
2024-04-22,04:46:36 | INFO | Compiling model...
[rank1]:[2024-04-22 04:51:45,423] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
2024-04-22,04:51:45 | INFO | Train Epoch: 0 [    512/1001472 (0%)] Data (t): 2.265 Batch (t): 306.846, 1.66859/s, 0.417148/s/gpu LR: 0.000000 Logit Scale: 14.286 Contrastive_loss: 6.4484 (6.4484) Caption_loss: 21.609 (21.609) Distill_loss: 0.0000 (0.0000) Loss: 28.058 (28.058)
2024-04-22,04:51:45 | INFO | Updating SWA model
2024-04-22,04:51:45 | INFO | Updating SWA model
2024-04-22,04:51:45 | INFO | Updating SWA model
2024-04-22,04:51:45 | INFO | Updating SWA model
2024-04-22,04:51:46 | INFO | SWA model updated
[rank0]:[2024-04-22 04:51:46,721] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
2024-04-22,04:51:47 | INFO | SWA model updated
2024-04-22,04:51:47 | INFO | SWA model updated
2024-04-22,04:51:47 | INFO | SWA model updated
[rank1]:[2024-04-22 04:51:47,268] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
[rank2]:[2024-04-22 04:51:47,272] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
[rank3]:[2024-04-22 04:51:47,298] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
2024-04-22,04:52:47 | INFO | Updating SWA model
2024-04-22,04:52:47 | INFO | Updating SWA model
2024-04-22,04:52:47 | INFO | Updating SWA model
2024-04-22,04:52:47 | INFO | Train Epoch: 0 [  51712/1001472 (5%)] Data (t): 0.065 Batch (t): 0.615, 858.193/s, 214.548/s/gpu LR: 0.000025 Logit Scale: 14.282 Contrastive_loss: 6.2008 (6.3246) Caption_loss: 20.075 (20.842) Distill_loss: 0.0000 (0.0000) Loss: 26.276 (27.167)
2024-04-22,04:52:47 | INFO | Updating SWA model
2024-04-22,04:52:47 | INFO | SWA model updated
2024-04-22,04:52:47 | INFO | SWA model updated
2024-04-22,04:52:47 | INFO | SWA model updated
2024-04-22,04:52:47 | INFO | SWA model updated
