{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62425edc-4d32-4df0-8019-50f81faa12c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28994c89-11e9-45a4-82a1-46429addb4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = {\n",
    "    \"base\": \"Baseline\",\n",
    "    \"elr\": \"ELR\",\n",
    "    \"sel\": \"Selective Loss\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c01fa383-587a-475b-8fcc-154a80f6d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Epoch: (\\d+) $$ ?\\d+/\\d+ $(\\d+)%$$$\n",
    "# image_to_text_mean_rank: ([\\d.]+)\\simage_to_text_median_rank: ([\\d.]+)\\simage_to_text_R@1: ([\\d.]+)\\simage_to_text_R@5: ([\\d.]+)\\simage_to_text_R@10: ([\\d.]+)\\stext_to_image_mean_rank: ([\\d.]+)\\stext_to_image_median_rank: ([\\d.]+)\\stext_to_image_R@1: ([\\d.]+)\\stext_to_image_R@5: ([\\d.]+)\\stext_to_image_R@10: ([\\d.]+)\\sclip_val_loss: ([\\d.]+).*val_generative_loss: ([\\d.]+)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad1d91bb-bf24-4f5d-8546-07f369976912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "contrastive_loss = {exp: [] for exp in exps}\n",
    "caption_loss = {exp: [] for exp in exps}\n",
    "# Eval\n",
    "clip_loss = {exp: [] for exp in exps}\n",
    "generative_loss = {exp: [] for exp in exps}\n",
    "rank_keys = [\"mean\", \"median\", \"1\", \"5\", \"10\"]\n",
    "img_text_metrics = {f\"img_text_{rk}\": {exp: [] for exp in exps} for rk in rank_keys}\n",
    "text_img_metrics = {f\"text_img_{rk}\": {exp: [] for exp in exps} for rk in rank_keys}\n",
    "imagenet_metrics = {f\"top{rk}\": {exp: [] for exp in exps} for rk in [1, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18c469d8-9a42-4982-a903-390468231497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_out_log(file_path, exp):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(len(lines)):\n",
    "            if \"(100%)\" in lines[i] and \"Train\" in lines[i]:\n",
    "                losses = re.search(r\"Contrastive_loss: ([\\d.]+).*Caption_loss: ([\\d.]+)\", lines[i])\n",
    "                contrastive_loss[exp].append(int(losses.group(1)))\n",
    "                caption_loss[exp].append(int(losses.group(2)))\n",
    "            elif \"@\" in lines[i]:\n",
    "                if not eval_losses:\n",
    "                    print(\"Darn\")\n",
    "                    print(lines[i])\n",
    "                for rk in rank_keys:\n",
    "                    if rk.isnumeric():\n",
    "                        img_text_metrics[f\"img_text_{rk}\"][exp].append(int(re.search(r\"image_to_text_R@{}: ([\\d.]+)\".format(rk), lines[i]).group(1)))\n",
    "                        text_img_metrics[f\"text_img_{rk}\"][exp].append(int(re.search(r\"text_to_image_R@{}: ([\\d.]+)\".format(rk), lines[i]).group(1)))\n",
    "                    else:\n",
    "                        img_text_metrics[f\"img_text_{rk}\"][exp].append(int(re.search(r\"image_to_text_{}_rank: ([\\d.]+)\".format(rk), lines[i]).group(1)))\n",
    "                        text_img_metrics[f\"text_img_{rk}\"][exp].append(int(re.search(r\"text_to_image_{}_rank: ([\\d.]+)\".format(rk), lines[i]).group(1)))\n",
    "                clip_loss[exp].append(int(re.search(r\"clip_val_loss: ([\\d.]+)\", lines[i]).group(1)))\n",
    "                generative_loss[exp].append(int(re.search(r\"val_generative_loss: ([\\d.]+)\", lines[i]).group(1)))\n",
    "                if \"imagenet\" in lines[i]:\n",
    "                    for rk in [1, 5]:\n",
    "                        imagenet_metrics[f\"top{rk}\"][exp].append(int(re.search(r\"-val-top{}: ([\\d.]+)\".format(rk), lines[i]).group(1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd21fa07-2ea8-4f69-9095-401b86e25400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chkpt(file_path, exp):\n",
    "    checkpoint = torch.load(file_path)\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    \n",
    "model_state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# Load the state dict into the model (assuming model is predefined and has the same architecture)\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "# Extract the optimizer state dictionary\n",
    "optimizer_state_dict = checkpoint['optimizer_state_dict']\n",
    "\n",
    "# Load the state dict into the optimizer (assuming optimizer is predefined and set up with the same parameters)\n",
    "optimizer.load_state_dict(optimizer_state_dict)\n",
    "\n",
    "# Extract the loss (if saved)\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "# Print the loaded information (optional)\n",
    "print(f\"Loaded checkpoint for epoch {epoch} with loss {loss}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b266cdef-0259-483a-bb57-e3f12e09ce9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79196b83-0d1c-424e-824f-5cf42f3e6bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b7a7b-1016-4529-b5c2-12847fbc15c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3fcdd0-e914-4c2e-b353-61e2ee43e42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d2cc22-35bb-4283-8aa2-ec6c9ff8f81c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "782e9931-1a1f-4179-8bce-1e281c51e1de",
   "metadata": {},
   "source": [
    "Certainly! To explain these concepts effectively to a graduate-level class, it's crucial to start with foundational ideas and then build towards more complex and technical descriptions. Here’s how you can structure your explanation for **contrastive loss**, **caption loss**, **generative loss**, and **CLIP loss**:\n",
    "\n",
    "### 1. **Contrastive Loss**\n",
    "\n",
    "#### Basic Concept:\n",
    "Contrastive loss is used in scenarios where the goal is to learn by comparing—specifically, to distinguish between similar and dissimilar items. In the context of machine learning, particularly in tasks involving embeddings (representations), the contrastive loss function helps to ensure that similar items are mapped closer together and dissimilar items are mapped farther apart in the embedding space.\n",
    "\n",
    "#### Technical Details:\n",
    "A common use of contrastive loss is in siamese networks or triplet networks, often used for tasks such as face verification or any form of learning from pairs. In these setups:\n",
    "- **Siamese networks** involve pairs of inputs. The loss calculates the distance between these pairs, pushing the distances of \"positive\" pairs (similar items) to be small, and \"negative\" pairs (dissimilar items) to be large.\n",
    "- **Triplet networks** extend this by using three inputs: an anchor, a positive (similar to the anchor), and a negative (dissimilar to the anchor). The loss ensures the distance between the anchor and the negative is greater than the distance between the anchor and the positive by some margin.\n",
    "\n",
    "The mathematical formulation often looks like this for a pair (x1, x2) with a binary label y indicating if they are similar (1) or not (0):\n",
    "$$ L = y \\cdot D(x1, x2)^2 + (1 - y) \\cdot \\max(0, m - D(x1, x2))^2 $$\n",
    "where $ D $ is a distance function (like Euclidean distance), and $ m $ is a margin enforced between dissimilar pairs.\n",
    "\n",
    "### 2. **Caption Loss**\n",
    "\n",
    "#### Basic Concept:\n",
    "Caption loss is typically found in image captioning tasks, where the model generates textual descriptions for images. The caption loss measures how well the generated text matches the expected text, helping to guide the training of models in generating accurate and relevant descriptions.\n",
    "\n",
    "#### Technical Details:\n",
    "Caption loss is commonly implemented using cross-entropy loss, which quantitatively measures the difference between the predicted probability distribution (generated caption) and the actual distribution (true caption). Cross-entropy loss is favored because it effectively handles the probabilities of a sequence of words:\n",
    "$$ L = -\\sum_{t=1}^T \\log(p_{target_t}) $$\n",
    "Here, $ T $ is the length of the caption, and $ p_{target_t} $ is the probability assigned by the model to the target word at position $ t $.\n",
    "\n",
    "### 3. **Generative Loss**\n",
    "\n",
    "#### Basic Concept:\n",
    "Generative loss applies to generative models, which are designed to generate new data instances that resemble the training data. This could be anything from images, text, or even new music. The generative loss measures how well the model performs this task, often focusing on how realistically the model replicates the data distribution.\n",
    "\n",
    "#### Technical Details:\n",
    "In generative adversarial networks (GANs), for example, the generative loss often involves a component where the generator tries to minimize the ability of a discriminator to distinguish generated data from real data, effectively minimizing:\n",
    "$$ \\log(1 - D(G(z))) $$\n",
    "where $ D $ is the discriminator, $ G $ is the generator, and $ z $ is a noise vector. The generator's loss is typically balanced against the discriminator's loss in a zero-sum game framework.\n",
    "\n",
    "### 4. **CLIP Loss (Contrastive Language–Image Pre-training)**\n",
    "\n",
    "#### Basic Concept:\n",
    "CLIP loss comes from the CLIP model by OpenAI, which learns visual concepts from natural language descriptions. It uses a contrastive loss formulation to align the text and image in a shared multidimensional space, promoting similarity between corresponding text and image pairs versus non-corresponding pairs.\n",
    "\n",
    "#### Technical Details:\n",
    "CLIP models are trained on a variety of images and text pairs. The loss function aims to maximize the cosine similarity between the correct pairs of images and texts compared to incorrect ones, using a temperature-scaled cross-entropy loss:\n",
    "$$ L = -\\log \\frac{\\exp(\\text{sim}(i, t) / \\tau)}{\\sum_{j=1}^N \\exp(\\text{sim}(i, j) / \\tau)} $$\n",
    "Here, $ \\text{sim}(i, t) $ represents the similarity score (e.g., cosine similarity) between the embeddings of the image $ i $ and text $ t $, and $ \\tau $ is a temperature parameter that scales the logits.\n",
    "\n",
    "### Conclusion for Presentation\n",
    "Start with the foundational concepts and the purpose of each loss type in specific tasks (contrast for learning\n",
    "\n",
    " discriminative features, captioning for matching descriptions, generative for creating new instances, and CLIP for aligning cross-modal data). Then delve into the mathematical expressions to show how these objectives are quantitatively implemented and optimized. This structure helps build understanding from ground up, connecting practical objectives with theoretical formulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beae354-75fb-4fd8-8041-62ceb6cc0cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
